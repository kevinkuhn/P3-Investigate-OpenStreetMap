{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:#BBBBBB;text-align:right;font-size:11px;\">Kevin Kuhn, 27.12.2015<br />Data Analyst Nanodegree<br />\n",
    "Nanodegree Program</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P3: Wrangle OpenStreetMap Data - Lucerne, Switzerland\n",
    "<a href=\"https://github.com/kevinkuhn/P3-Investigate-OpenStreetMap/\" target=\"_blank\">https://github.com/kevinkuhn/P3-Investigate-OpenStreetMap/</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process your dataset\n",
    "What to do: <i>Thoroughly audit and clean your dataset, converting it from XML to JSON format. It is recommended that you start with the Lesson 6 exercises and modify them to suit your chosen dataset. As you unravel the data, take note of problems encountered along the way as well as issues with the dataset. You are going to need these when you write your project report. Finally, import the clean JSON file into a MongoDB database and run some queries against it.</i><br />\n",
    "1.\tAudit Data<br />\n",
    "2.\tClean Dataset & convert XML to JSON<br />\n",
    "3.\tImport clean JSON file into MongoDB<br />\n",
    "4.\tRun Queries<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lucerne, Switzerland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen to work on the city map of Lucerne, Switzerland – my home town.<br />\n",
    "<img src=\"data/lucerne.png\" alt=\"Lucerne\" />\n",
    "\n",
    "Wikipedia: https://en.wikipedia.org/wiki/Lucerne<br />\n",
    "OpenStreetMap: http://www.openstreetmap.org/export#map=12/47.0508/8.3064<br /><br />With 118 MB (124,599,310 Bytes) the file size meets the specifications of at least 50MB. The map has been downloading by using Overpass API.<br />\n",
    "<img src=\"data/map-lucerne.png\" alt=\"Lucerne Map\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems encountered in the dataset\n",
    "### 1. Street names are different in Switzerland than in other countries like the US\n",
    "In Switzerland street names are not separated by whitespace. Street names always contain the street attribute. For example: Street in German is called \"Strasse\" and a street like Main Street in US would be called \"Haupt*strasse*\" in Switzerland. The \"*strasse*\" represents the street. So therefore I will have to rewrite this functions that it checks if \"strasse\" appears within the steet_name.I recognized this problem when I tried to run a function that searches for expected road name endings. After a few hours of testing and working on the function I had to rewrite the whole function so that it checked if the expected street name ending was within the word and not split it and then check.\n",
    "### 2. Data encoding on Windows\n",
    "One of the biggest problems I encountered was not with the data but with my computer and it's encoding. As I work on Windows 10 my console has another default encoding as utf-8. My dataset contained characters like äöü that would only be displayed in utf-8. Therefor the console caued a lot of problems which took me hours to figure out how to fix them. Every print command with these special characters cauesed an error so that I had to change my registry files to fix it. But this then caused problems with other systems running on my computer. At the end I knew that if I do not use the print command I can run the scripts so I tried not to use the print command to often and it worked. In the future I will consider working with a Mac or a Linux system because I will encounter these problems for sure again.\n",
    "### 3. Creating JSON file\n",
    "As the file contains a lot of information it was first hard for me to figure out which information belongs to which node. I had to investigate a lot of time in figuring out how to transform an XML file to a dictionary and then save the dictionary correctly into a JSON file. With a lot of tutorials, some stackflow and github researches I finally managed to pass the data and save them localy into an .json file.\n",
    "### 4. Problems within the dataset\n",
    "I have to say that I did not meet a lot of problems with the dataset. It was very clean. By running the queries I saw a few entries that I knew from \"real life\" and what was described was not always correctly but this had nothing to do with the logic of the data itself. The street names I checked as well as the postcodes where all formatted correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Audit Data\n",
    "The XML data will pass three audits:\n",
    "1. street names\n",
    "2. restaurants\n",
    "3. postcodes\n",
    "\n",
    "### Steet name audit\n",
    "The street name audit checked whether a street name contains one of the expected street name endings or not. If not, the steet name will be added to a list and checked for wrong spellings. These spellings will then be autocorrected by the system.<br />\n",
    "\n",
    "The street name audit showed that the dataset was very clean and contained almost no errors for street maps. In order to check other datasets some basic, possible mistakes have been mapped:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "number of entries without street name:  628639\n",
    "number of entries with street name:  11365\n",
    "number of entries with street name that contains 'Strasse':  6154\n",
    "number of entries with street name that contains  strasse :  6032\n",
    "number of entries with street name that contains  Strasse :  122\n",
    "number of entries with street name that contains  platz :  86\n",
    "number of entries with street name that contains  weg :  882\n",
    "number of entries with street name that contains  gasse :  167\n",
    "number of entries with street name that contains  str. :  6103\n",
    "number of entries with street name that contains  matt :  1365\n",
    "number of entries with street name that contains  matte :  416\n",
    "number of entries with street name that contains  halde :  232\n",
    "number of entries with street name that contains  quai :  57\n",
    "number of entries with street name that contains  ring :  339\n",
    "number of entries with street name that contains  hof :  631\n",
    "number of entries with street name that contains  weid :  261\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>\n",
    "mapping = { u\"str.\" : u\"strasse\",\n",
    "            u\"Alee\": u\"Allee\",\n",
    "            u\"Alle\": u\"Allee\",\n",
    "            u\"gäsli\" : u\"gässli\",\n",
    "            u\"dorv\" : u\"dorf\",\n",
    "            u\"gase\" : u\"gasse\",\n",
    "            u\"veg\" : u\"weg\",\n",
    "            u\"höffli\" : u\"höfli\",\n",
    "            u\"rein\" : u\"rain\",\n",
    "            u\"paradiesgässli\" : u\"Paradiesgässli\"\n",
    "            }\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/changes-street-name.png\" width=\"320px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant audit\n",
    "The restaurant audit did the same job as the sreet name audit. By looking trough the data some miss spelled restaurant names have been found. this have been cleaned out by the system.<br />\n",
    "<img src=\"data/restaurant-audit.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postcodes audit\n",
    "The postcodes audit's task is to compare the postcodes dictonary from an authorized distributor (post office directory Switzerland) with the information in the dataset. Therefore a CSV file will be imported and transformed to a dictonary which will be compared with the dataset.<br />\n",
    "<img src=\"data/postcodes-audit.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert XML to JSON\n",
    "Converting the modified XML file to JSON happened by using the data.py script. The script shapes the elements from the XML file to fit into JSON format. By converting the XML file to JSON the file name will be named the same as the XML file, just adding the .json extension.<br />\n",
    "<img src=\"data/osm-json-file.png\" width=\"650px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import clean JSON file into MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The created JSON file from step #2 will be imported by command. The collection the data will be imported is called lucerne. This collection will be used for the further analysis. The JSON file (~185MB) import took about 15 seconds. During this time 640'004 documents have been created where 76'198 of them are tags with the attibute \"way\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>Executing: mongoimport --db osm --collection lucerne --file source/lucerne.osm.json\n",
    "\n",
    "2015-12-26T11:57:30.290+0100    connected to: localhost\n",
    "2015-12-26T11:57:33.286+0100    [####....................] osm.lucerne  30.8 MB/181.2 MB (17.0%)\n",
    "2015-12-26T11:57:36.286+0100    [########................] osm.lucerne  63.5 MB/181.2 MB (35.0%)\n",
    "2015-12-26T11:57:39.288+0100    [############............] osm.lucerne  94.6 MB/181.2 MB (52.2%)\n",
    "2015-12-26T11:57:42.286+0100    [################........] osm.lucerne  125.3 MB/181.2 MB (69.2%)\n",
    "2015-12-26T11:57:45.286+0100    [#####################...] osm.lucerne  162.7 MB/181.2 MB (89.8%)\n",
    "2015-12-26T11:57:46.907+0100    [########################] osm.lucerne  181.2 MB/181.2 MB (100.0%)\n",
    "2015-12-26T11:57:46.910+0100    imported 640004 documents\n",
    "\n",
    "Number of nodes: 563776\n",
    "Number of ways 76198</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Queries\n",
    "To run the queries agains MongoDB queries with projection had been created:\n",
    "<img src=\"data/query.png\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this query is <code>[]</code> which shows that there is no vegetarian restaurant inside the database. The same for \"vegan\" restaurants which is true and sad at the same time!\n",
    "<img src=\"https://media.giphy.com/media/5AVgmIw7iAzdK/giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which cuisine is the most frequent in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>[{u'_id': u'regional', u'count': 53},\n",
    " {u'_id': u'italian', u'count': 37},\n",
    " {u'_id': u'chinese', u'count': 11},\n",
    " {u'_id': u'thai', u'count': 11},\n",
    " ...]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which postcode is the most frequent in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>[{u'_id': u'6005', u'count': 1105},\n",
    " {u'_id': u'6023', u'count': 1022},\n",
    " {u'_id': u'6375', u'count': 753},\n",
    " {u'_id': u'6037', u'count': 652},\n",
    " ...]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which city is the most frequent in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>[{u'_id': u'Root', u'count': 656},\n",
    " {u'_id': u'Luzern', u'count': 642},\n",
    " {u'_id': u'Nottwil', u'count': 612},\n",
    " {u'_id': u'Sempach', u'count': 611},\n",
    " ...]\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check which user is the most active that edited the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>[{u'count': 255141, u'_id': u'FischersFritz'},\n",
    "{u'count': 94773, u'_id': u'aMuTeX'},\n",
    "{u'count': 30874, u'_id': u'Bullroarer'},\n",
    "{u'count': 22841, u'_id': u'Pudu'},\n",
    "{u'count': 16048, u'_id': u'Roadrunner21'},\n",
    "{u'count': 14256, u'_id': u'lumapper'},\n",
    "{u'count': 13948, u'_id': u'Andy9FromSpace'}, \n",
    "{u'count': 11254, u'_id': u'mdk'},\n",
    "{u'count': 10643, u'_id': u'Stans'},\n",
    "{u'count': 7717, u'_id': u'valivali999'}]</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other interesting facts from the query search you can find in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of the Data\n",
    "\n",
    "The data I have analysed contains the dataset of Lucerne, Switzerland. The OSM/XML file has more than 640'000 entries which showed some interesting information after running the queries.\n",
    "\n",
    "Lucerne has\n",
    "* 79 registered/public toilets\n",
    "* Postcode 6005 has the most entries\n",
    "* 301 wheelchair friendly places\n",
    "* FischersFritz is the most active user. He deployed 255141 entries/changes\n",
    "* mostly regional cuisines\n",
    "* not very vegetarian/vegan friendly place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ideas about the datasets\n",
    "The implementation of this analysis has the following limitations:\n",
    "#### 1. Country\n",
    "This script is fully optimized for German speaking regions. Countries like Germany and Austria could also use this script. Would this script however be used by non German speaking countries it could cause problems, especially regarding the mapping (see in #3).\n",
    "\n",
    "#### 2. Postcodes\n",
    "To check the postcodes within the dataset with the postcodes defined by the Swiss Government it woul be needed to consult multiple datasets and not only the one for Lucerne. This would be needed to done by an API to dynamicly access the files you need for your automatization.\n",
    "\n",
    "#### 3. Other languages\n",
    "Switzerland has 4 official languages which could cause difficulties by using the mapping element. Mapping would needed to be translated and regional specification would have been considered. Additionally each language has different codes which would need little changes on the functions. This script aims only for fixing german speaking parts of Switzerland.\n",
    "\n",
    "#### 4. Performance\n",
    "Regarding the duration it took me to run the scripts on my local machine it would take a way faster machine to calculate with a bigger dataset. For sure this task would have to be transfered to a server where it would be automized by an API and high performance service/server.\n",
    "\n",
    "#### 5. Encoding\n",
    "In order to provide this service in a professional matter it would need to ensure to use the right codec and that it would cause no problems by running true functions from imported libraries or limitations set by the system the script runs on.\n",
    "\n",
    "#### 6. Security\n",
    "To make this service available beyond local machines and local files it would need to create a high security environment. The connection to the database of MongoDB as well as all the other connection where information will be passed have to be secured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible usage\n",
    "With the dataset it would be possible to provide a service for handicapped people where they can see which location is wheelchair friendly. Another service could be to show restaurants you can find near to your location, separated by type of restaurant. Another possible usage of the data could be for an app to show public toilets near to a location.\n",
    "There are endless possiblities of what to do with this data and I would say that there are no limits. My personal interest would be in returning the data to a lot of people to get feedback if the data is still correct and then create a machine that passes data that has not been modified for a long time to \"check it up\" if it is still right and then teach the machine to compare patterns out of the responses and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personal conclusion\n",
    "In the project before - P2 Investigate a Dataset - I had a few troubles with python and now I finally see the progress I have made in working with Python. This exercise helped me a lot and I am excited to dig deeper and deeper into the data analysis. My personal challenge was to figure out in which way I had to structure the data/scripts and how to use Git to work localy. I am happy with the result and excited about the feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Python File Description\n",
    "#### <code>data.py</code>\n",
    "Creating a JSON file out of OSM/XML data\n",
    "#### <code>mongodb-import-queries.py</code>\n",
    "Connection to MongoDB, Importing JSON, Running Queries against the database\n",
    "#### <code>postcode-audit.py</code>\n",
    "Loading CSV file, Loading OSM/XML file, comparing postcodes from CSV list with those in the XML file, replacing wrong entries\n",
    "#### <code>restaurant-audit.py</code>\n",
    "Loading OSM/XML file, checking for the different restaurant names, replacing wrong spelled names\n",
    "#### <code>street-name-audit.py</code>\n",
    "Loading OSM/XML file, checking for street names containing unexpected characters, replacing wrong spelled names\n",
    "#### <code>visualize.py</code>\n",
    "Script to create pie charts for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "<p style=\"font-size:12px;\">db.createCollection()<br/>\n",
    "https://docs.mongodb.org/v3.0/reference/method/db.createCollection/<br/><br/>\n",
    "Encodings in Python (German)<br/>\n",
    "http://gelb.bcom.at/trac/misc/wiki/TutorialsPython/PythonUmlauteUnicodeEncodings<br/><br/>\n",
    "How do I use the 'json' module to read in one JSON object at a time?<br/>\n",
    "http://stackoverflow.com/questions/21708192/how-do-i-use-the-json-module-to-read-in-one-json-object-at-a-time/21709058#21709058<br/><br/>\n",
    "How to check a string for specific characters?<br/>\n",
    "http://stackoverflow.com/questions/5188792/how-to-check-a-string-for-specific-characters<br/><br/>\n",
    "Import Example Dataset<br/>\n",
    "https://docs.mongodb.org/getting-started/shell/import-data/<br/><br/>\n",
    "Introduction to working with MongoDB and PyMongo<br/>\n",
    "http://api.mongodb.org/python/current/tutorial.html<br/><br/>\n",
    "JSON File Validator<br/>\n",
    "http://jsonlint.com/<br/><br/>\n",
    "Loading and parsing a JSON file in Python<br/>\n",
    "http://stackoverflow.com/questions/12451431/loading-and-parsing-a-json-file-in-python/12451465#12451465<br/><br/>\n",
    "OpenSteetMap Data Lucerne, Switzerland<br/>\n",
    "http://www.openstreetmap.org/relation/1682891#map=12/47.0408/8.3173<br/><br/>\n",
    "PEP 0263 -- Defining Python Source Code Encodings<br/>\n",
    "https://www.python.org/dev/peps/pep-0263/<br/><br/>\n",
    "Postcodes Lucerne, Lucerne, Switzerland<br/>\n",
    "https://www.post.ch/db/owa/pv_plz_pack/pr_street?p_cantone=&p_language=de&p_username=null&p_localita_search=Luzern&p_localita_in=Luzern&p_tipo=luogo&p_nap=&p_strada=<br/><br/>\n",
    "Prevent encoding errors in Python<br/>\n",
    "http://stackoverflow.com/questions/11050292/prevent-encoding-errors-in-python<br/><br/>\n",
    "Python - get position in list<br/>\n",
    "http://stackoverflow.com/questions/364621/python-get-position-in-list<br/><br/>\n",
    "Python String replace() Method<br/>\n",
    "http://www.tutorialspoint.com/python/string_replace.htm<br/><br/>\n",
    "Python: UnicodeEncodeError: ‘ascii’ codec can’t encode character u’\\xfc’ in position 11: ordinal not in range(128)<br/>\n",
    "http://www.markhneedham.com/blog/2015/05/21/python-unicodeencodeerror-ascii-codec-cant-encode-character-uxfc-in-position-11-ordinal-not-in-range128/<br/><br/>\n",
    "Replace part of a string in Python?<br/>\n",
    "http://stackoverflow.com/questions/10037742/replace-part-of-a-string-in-python<br/><br/>\n",
    "Save plot to image file instead of displaying it using Matplotlib<br/>\n",
    "http://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib-so-it-can-be<br/><br/>\n",
    "Saving Figures From Pyplot<br/>\n",
    "http://www.jesshamrick.com/2012/09/03/saving-figures-from-pyplot/<br/><br/>\n",
    "Tutorial MongoDB and PyMongo<br/>\n",
    "http://api.mongodb.org/python/current/tutorial.html<br/><br/>\n",
    "UnicodeDecodeError<br/>\n",
    "https://wiki.python.org/moin/UnicodeDecodeError</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kevin Kuhn, 27.12.2015<br />\n",
    "<a href=\"https://ch.linkedin.com/in/kuhnkevin\" target=\"_blank\">LinkedIn</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
